{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "Torchvision version: 0.18.1+cu121\n",
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from monai.metrics import DiceMetric, MeanIoU, SurfaceDiceMetric, SSIMMetric, GeneralizedDiceScore\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.losses import GeneralizedDiceLoss, DiceLoss, GeneralizedDiceFocalLoss\n",
    "from monai.metrics import DiceMetric, GeneralizedDiceScore\n",
    "#from LinearWarmupCosine import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    polygons = []\n",
    "    color = []\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:,:,i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/39327/Desktop/SAM/DataSet/CVC-300/images\\208.png C:/Users/39327/Desktop/SAM/DataSet/CVC-300/masks\\208.png\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"C:/Users/39327/Desktop/SAM/DataSet/CVC-300/images\"\n",
    "mask_folder =  \"C:/Users/39327/Desktop/SAM/DataSet/CVC-300/masks\"\n",
    "save_folder = \"C:/Users/39327/Desktop/SAM/DataSet/CVC-300/dataset\"\n",
    "os.makedirs(save_folder, exist_ok = True)\n",
    "\n",
    "image_path = []\n",
    "mask_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(image_folder, topdown=False): #finds MRI files\n",
    "    for name in files:\n",
    "        if name.endswith(\".png\"):\n",
    "            apath=os.path.join(root, name)\n",
    "            image_path.append(apath)\n",
    "            \n",
    "for root, dirs, files in os.walk(mask_folder, topdown=False): #finds MRI files\n",
    "    for name in files:\n",
    "        if name.endswith(\".png\"):\n",
    "            apath=os.path.join(root, name)\n",
    "            mask_path.append(apath)\n",
    "            \n",
    "print(image_path[-1], mask_path[-1])\n",
    "\n",
    "# with open('D:\\Yuheng Li\\Segment Anything\\kvasir-seg\\\\kavsir_bboxes.json') as f:\n",
    "#     labels = json.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_path, mask_path, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "# model_type = \"vit_h\"\n",
    "\n",
    "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "# sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "# model_type = \"vit_l\"\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "transform = ResizeLongestSide(sam.image_encoder.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_bboxes(mask, num_instances):\n",
    "\n",
    "#     \"\"\"Compute bounding boxes from masks.\n",
    "\n",
    "#     mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
    "\n",
    " \n",
    "\n",
    "#     Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     boxes = np.zeros([num_instances, 4], dtype=np.int32)\n",
    "\n",
    "#     for i in range(num_instances):\n",
    "\n",
    "#         m = mask\n",
    "\n",
    "#         # Bounding box.\n",
    "\n",
    "#         horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
    "\n",
    "# #         print(\"np.any(m, axis=0)\",np.any(m, axis=0))\n",
    "\n",
    "# #         print(\"p.where(np.any(m, axis=0))\",np.where(np.any(m, axis=0)))\n",
    "\n",
    "#         vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
    "\n",
    "#         if horizontal_indicies.shape[0]:\n",
    "\n",
    "#             x1, x2 = horizontal_indicies[[0, -1]]\n",
    "\n",
    "#             y1, y2 = vertical_indicies[[0, -1]]\n",
    "\n",
    "#             # x2 and y2 should not be part of the box. Increment by 1.\n",
    "\n",
    "#             x2 += 1\n",
    "\n",
    "#             y2 += 1\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             # No mask for this instance. Might happen due to\n",
    "\n",
    "#             # resizing or cropping. Set bbox to zeros\n",
    "\n",
    "#             x1, x2, y1, y2 = 0, 0, 0, 0\n",
    "\n",
    "#         boxes[i] = np.array([y1, x1, y2, x2])\n",
    "        \n",
    "\n",
    "#     return boxes.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing bonding boxes with yolo\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define the function to convert YOLO bounding boxes to the required format\n",
    "def extract_bboxes_from_yolo(yolo_data):\n",
    "    \"\"\"\n",
    "    Compute bounding boxes from YOLO data.\n",
    "\n",
    "    yolo_data: Tensor with shape (num_instances, 4) containing detected bounding boxes.\n",
    "               Each bounding box is represented as [x_min, y_min, x_max, y_max].\n",
    "\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "    \"\"\"\n",
    "    num_instances = yolo_data.shape[0]\n",
    "    boxes = np.zeros([num_instances, 4], dtype=np.int32)\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        x_min, y_min, x_max, y_max = yolo_data[i, :4]\n",
    "        boxes[i] = np.array([y_min, x_min, y_max, x_max])\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(\"C:/Users/39327/runs/detect/train10/weights/best.pt\")\n",
    "\n",
    "# Dictionary to store bounding boxes for each image\n",
    "bbox_cache = {}\n",
    "\n",
    "def extract_bboxes(image, num_instances):\n",
    "    \"\"\"\n",
    "    Perform object detection on the image and extract bounding boxes.\n",
    "\n",
    "    image: The input image on which to perform object detection.\n",
    "\n",
    "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
    "             If no boxes are detected, returns a bounding box that covers the entire image.\n",
    "    \"\"\"\n",
    "    # Generate a unique identifier for the image (e.g., using image data hash)\n",
    "    image_id = hash(image.data.tobytes())\n",
    "\n",
    "    # Check if bounding boxes for this image are already cached\n",
    "    if image_id in bbox_cache:\n",
    "        print(\"Using cached bounding boxes.\")\n",
    "        return bbox_cache[image_id]\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(image, device='cuda', conf=0.2)\n",
    "\n",
    "    if not results:  # If results list is empty\n",
    "        height, width = image.shape[:2]\n",
    "        bbox_cache[image_id] = np.array([[0, 0, height, width]], dtype=np.int32)\n",
    "        return bbox_cache[image_id]\n",
    "    \n",
    "    result = results[0]  # Assuming only one result is returned for one image\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "\n",
    "    # Extract the 'xyxy' attribute\n",
    "    xyxy = boxes.xyxy.cpu().numpy()  # Convert tensor to numpy array\n",
    "\n",
    "    if xyxy.size == 0:  # No boxes detected\n",
    "        height, width = image.shape[:2]\n",
    "        bbox_cache[image_id] = np.array([[0, 0, height, width]], dtype=np.int32)\n",
    "        return bbox_cache[image_id]\n",
    "\n",
    "    # Convert to the required format\n",
    "    extracted_boxes = extract_bboxes_from_yolo(xyxy)\n",
    "    \n",
    "    #Calculate shift amounts\n",
    "    # shift_x = image.shape[1] * 0.13  # 20% of the image width\n",
    "    # shift_y = image.shape[0] * 0.1   # 10% of the image height\n",
    "    \n",
    "    # #Shift boxes 20% to the right and 10% down\n",
    "    # extracted_boxes[:, 1] = np.clip(extracted_boxes[:, 1] + shift_x, 0, image.shape[1])  # Shift x1\n",
    "    # extracted_boxes[:, 3] = np.clip(extracted_boxes[:, 3] + shift_x, 0, image.shape[1])  # Shift x2\n",
    "    # extracted_boxes[:, 0] = np.clip(extracted_boxes[:, 0] + shift_y, 0, image.shape[0])  # Shift y1\n",
    "    # extracted_boxes[:, 2] = np.clip(extracted_boxes[:, 2] + shift_y, 0, image.shape[0])  # Shift y2\n",
    "    \n",
    "    # # Increase box size by 10%\n",
    "    # for i in range(extracted_boxes.shape[0]):\n",
    "    #     y1, x1, y2, x2 = extracted_boxes[i]\n",
    "    #     box_width = x2 - x1\n",
    "    #     box_height = y2 - y1\n",
    "        \n",
    "    #     # Increase size by 10%\n",
    "    #     new_width = box_width * 1.15\n",
    "    #     new_height = box_height * 1.1\n",
    "        \n",
    "    #     # Calculate the new coordinates\n",
    "    #     new_x1 = x1 - (new_width - box_width) / 2\n",
    "    #     new_y1 = y1 - (new_height - box_height) / 2\n",
    "    #     new_x2 = x2 + (new_width - box_width) / 2\n",
    "    #     new_y2 = y2 + (new_height - box_height) / 2\n",
    "        \n",
    "    #     # Ensure the new coordinates are within the image boundaries\n",
    "    #     new_x1 = np.clip(new_x1, 0, image.shape[1])\n",
    "    #     new_y1 = np.clip(new_y1, 0, image.shape[0])\n",
    "    #     new_x2 = np.clip(new_x2, 0, image.shape[1])\n",
    "    #     new_y2 = np.clip(new_y2, 0, image.shape[0])\n",
    "        \n",
    "    #     extracted_boxes[i] = [new_y1, new_x1, new_y2, new_x2]\n",
    "\n",
    "    # Store the bounding boxes in the cache\n",
    "    bbox_cache[image_id] = extracted_boxes\n",
    "\n",
    "    #print(\"Extracted boxes:\", extracted_boxes)\n",
    "    return extracted_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, image_size):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # TODO: use ResizeLongestSide and pad to square\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.image_path[index].split('images\\\\')[1].split('.png')[0]\n",
    "\n",
    "        image = cv2.imread(self.image_path[index])\n",
    "        gt = cv2.imread(self.mask_path[index])\n",
    "        gt = cv2.cvtColor(gt, cv2.COLOR_BGR2GRAY) / 255\n",
    "        gt = gt.astype('float32')\n",
    "\n",
    "        #bbox_arr = extract_bboxes(gt, 1)\n",
    "        #to do: extract bounding box from yolo\n",
    "        bbox_arr = extract_bboxes(image, 1)\n",
    "\n",
    "        gt_resized = cv2.resize(gt, (1024, 1024), cv2.INTER_NEAREST)\n",
    "        gt_resized = torch.as_tensor(gt_resized > 0).long()\n",
    "        \n",
    "        gt = torch.from_numpy(gt)\n",
    "        gt_binary_mask = torch.as_tensor(gt > 0).long()\n",
    "\n",
    "        transform = ResizeLongestSide(self.image_size)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image =  cv2.resize(input_image, (1024, 1024), cv2.INTER_CUBIC)\n",
    "        input_image= self.to_tensor(input_image)\n",
    "        \n",
    "        # input_image= self.normalize(input_image)\n",
    "#         print(input_image.shape)\n",
    "#         plt.figure()\n",
    "#         plt.imshow(input_image[0])\n",
    "#         print('before preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))\n",
    "        # input_image = sam.preprocess(input_image.to('cuda:0')).detach().cpu()\n",
    "#         print('after preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))\n",
    "#         input_image = cv2.resize(input_image.numpy(), (1024, 1024), cv2.INTER_CUBIC)\n",
    "\n",
    "#         plt.figure()\n",
    "#         plt.imshow(input_image[0])\n",
    "        \n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(input_image.shape[-2:])\n",
    "        \n",
    "        return input_image, np.array(bbox_arr), gt_binary_mask, gt_resized, original_image_size, input_size\n",
    "    \n",
    "\n",
    "def my_collate(batch):\n",
    "    \n",
    "    images, bboxes, masks, gt_resized, original_image_size, input_size = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    gt_resized = torch.stack(gt_resized, dim=0)\n",
    "    \n",
    "    masks = [m for m in masks]\n",
    "    bboxes = [m for m in bboxes]\n",
    "    original_image_size = [m for m in original_image_size]\n",
    "    input_size = [m for m in input_size]\n",
    "    \n",
    "    return images, bboxes, masks, gt_resized, original_image_size, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_dataset = ColonDataset(X_test, y_test, sam.image_encoder.img_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x640 1 polyp, 72.7ms\n",
      "Speed: 3.0ms preprocess, 72.7ms inference, 69.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 39.5ms\n",
      "Speed: 4.0ms preprocess, 39.5ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 40.0ms\n",
      "Speed: 3.0ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 39.0ms\n",
      "Speed: 3.5ms preprocess, 39.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 39.5ms\n",
      "Speed: 4.4ms preprocess, 39.5ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 36.5ms\n",
      "Speed: 4.0ms preprocess, 36.5ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 36.6ms\n",
      "Speed: 4.0ms preprocess, 36.6ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 36.0ms\n",
      "Speed: 3.0ms preprocess, 36.0ms inference, 3.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 37.0ms\n",
      "Speed: 3.0ms preprocess, 37.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 35.5ms\n",
      "Speed: 4.0ms preprocess, 35.5ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 35.8ms\n",
      "Speed: 4.0ms preprocess, 35.8ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 1 polyp, 36.0ms\n",
      "Speed: 3.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Mean val dice: 0.9199980000654856\n",
      "Mean val gd: 0.8810161600510279\n",
      "Mean val iou: 0.8792454699675242\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:/Users/39327/CV/polyp/SAM-CVC/New folder/All datasets-20240524T154817Z-001/All datasets/SAM Finetune Enc Dec\"\n",
    "\n",
    "sam.prompt_encoder.load_state_dict(torch.load(os.path.join(model_path, \"prompt_enc_best_dice_model_DL.pth\")))\n",
    "sam.image_encoder.load_state_dict(torch.load(os.path.join(model_path, \"img_enc_best_dice_model_DL.pth\")))\n",
    "sam.mask_decoder.load_state_dict(torch.load(os.path.join(model_path, \"dec_best_dice_model_DL.pth\")))\n",
    "sam.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_dice = []\n",
    "    batch_gd = []\n",
    "    batch_iou = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]\n",
    "\n",
    "        dice = DiceMetric()\n",
    "        gd =  GeneralizedDiceScore()\n",
    "        iou = MeanIoU()\n",
    "\n",
    "        for i in range(len(mask)):\n",
    "            image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))\n",
    "\n",
    "            orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]\n",
    "            col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y\n",
    "            col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x\n",
    "\n",
    "            box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()\n",
    "\n",
    "            num_masks = box.shape[0]\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "              points=None,\n",
    "              boxes= box_torch,\n",
    "              masks = None\n",
    "            )\n",
    "\n",
    "            low_res_masks, iou_predictions = sam.mask_decoder(\n",
    "              image_embeddings=image_embedding,\n",
    "              image_pe=sam.prompt_encoder.get_dense_pe(),\n",
    "              sparse_prompt_embeddings=sparse_embeddings,\n",
    "              dense_prompt_embeddings=dense_embeddings,\n",
    "              multimask_output=False\n",
    "            )\n",
    "\n",
    "            upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i])\n",
    "\n",
    "            binary_mask = torch.sigmoid(upscaled_masks.detach().cpu())\n",
    "            binary_mask =  (binary_mask>0.5).float()\n",
    "\n",
    "            gt_binary_mask = mask[i].detach().cpu()\n",
    "\n",
    "            if binary_mask.size()[0] > 1:\n",
    "                binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)\n",
    "\n",
    "            dice.reset()\n",
    "            gd.reset()\n",
    "            iou.reset()\n",
    "\n",
    "            dice(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            gd(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            iou(binary_mask[0,:], gt_binary_mask.unsqueeze(0))\n",
    "            final_dice = dice.aggregate().numpy()[0]\n",
    "            final_gd = gd.aggregate().numpy()[0]\n",
    "            final_iou = iou.aggregate().numpy()[0]\n",
    "            batch_dice.append(final_dice)\n",
    "            batch_gd.append(final_gd)\n",
    "            batch_iou.append(final_iou)\n",
    "\n",
    "\n",
    "    print(f'Mean val dice: {sum(batch_dice) / len(batch_dice)}')\n",
    "    print(f'Mean val gd: {sum(batch_gd) / len(batch_gd)}')\n",
    "    print(f'Mean val iou: {sum(batch_iou) / len(batch_iou)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
